# Advance Statistics

## Data Exploration
Data exploration is the process of examining and analyzing data to discover patterns, relationships, and insights. It involves using various statistical and visualization techniques to understand the characteristics of the data, identify anomalies and outliers, and gain a deeper understanding of the underlying structure of the data.

## Data Preprocessing
### - Data Transformation
Data transformation refers to the process of converting raw data from one format, structure, or representation to another format that is more suitable for analysis or processing. The transformation process can involve a wide range of techniques, including data cleaning, data normalization, data aggregation, and feature engineering.

### - Feature Selection
Feature selection is the process of selecting a subset of relevant features (or variables) from a larger set of features in a dataset. The objective of feature selection is to reduce the dimensionality of the data and improve the accuracy and efficiency of machine learning models by eliminating irrelevant or redundant features that do not contribute much to the performance of the model.

### - Cross Validation
Cross-validation is a statistical method used to evaluate the performance of a machine learning model by dividing the available dataset into multiple subsets, or "folds." The model is trained on a subset of the data and tested on the remaining subset. This process is repeated multiple times with different subsets of the data, and the performance of the model is averaged over all the runs.

### - Principal Component Analysis
Principal Component Analysis (PCA) is a statistical technique used to transform a set of variables into a smaller set of linearly uncorrelated variables called principal components. These principal components are ranked in order of the amount of variation they explain in the original dataset, with the first principal component accounting for the most variation, followed by the second principal component, and so on.

## Model Building
### - Simple Linear Regression
Simple Linear Regression is a statistical method used to model the linear relationship between a dependent variable (Y) and an independent variable (X). The goal of Simple Linear Regression is to find a line of best fit that summarizes the relationship between the two variables.

### - Weighted Linear Regression
Weighted Linear Regression is a variant of Simple Linear Regression in which each data point is assigned a weight that reflects the importance or reliability of that point. The weights are used to give more emphasis to some data points and less emphasis to others when estimating the parameters of the regression model.

### - Partial Least Squares Regression
Partial Least Squares Regression (PLS Regression) is a statistical technique used for modeling the linear relationship between two sets of variables, X (predictors) and Y (response). PLS Regression is a multivariate technique that can handle situations where there are more predictors than observations or when the predictors are highly correlated with each other.

### - Pricipal Component Regression
Partial Least Squares Regression (PLS Regression) is a statistical technique used for modeling the linear relationship between two sets of variables, X (predictors) and Y (response). PLS Regression is a multivariate technique that can handle situations where there are more predictors than observations or when the predictors are highly correlated with each other.

### - Ridge Regression
Ridge Regression is a variant of linear regression that is used to mitigate the problem of multicollinearity (i.e., high correlation among the predictor variables) in multiple linear regression. In Ridge Regression, a penalty term is added to the least squares objective function to shrink the regression coefficients towards zero, thus reducing their variance.

### - Lasso Regression
Lasso Regression, short for Least Absolute Shrinkage and Selection Operator Regression, is a variant of linear regression that is used for variable selection and regularization. Like Ridge Regression, Lasso Regression can be used to mitigate the problem of multicollinearity in multiple linear regression.
